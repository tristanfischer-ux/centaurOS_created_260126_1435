ğŸš€ Ollama Proxy Server Starting...
ğŸ“ Proxy listening on: http://localhost:8000
ğŸ”— Forwarding to Ollama: http://localhost:11434

ğŸ“Š Model Mappings:
   gpt-4 â†’ qwen2.5:14b-instruct-q4_K_M
   gpt-4o â†’ qwen2.5:14b-instruct-q4_K_M
   gpt-4-turbo â†’ qwen2.5:14b-instruct-q4_K_M
   gpt-4-turbo-preview â†’ qwen2.5:14b-instruct-q4_K_M
   gpt-3.5-turbo â†’ qwen2.5-coder:7b
   gpt-3.5 â†’ qwen2.5-coder:7b
   claude-3-5-sonnet â†’ deepseek-coder:6.7b
   claude-3.5-sonnet â†’ deepseek-coder:6.7b
   claude-3-sonnet â†’ deepseek-coder:6.7b
   claude-3-opus â†’ qwen2.5:14b-instruct-q4_K_M
   claude-3-haiku â†’ qwen2.5-coder:7b

âœ… Proxy server is ready!

ğŸ”§ Now update Cursor settings:
   Base URL: http://localhost:8000/v1
   API Key: ollama

ğŸ’¡ Use any model from Cursor's dropdown - they'll work with your local models!
ğŸ“‹ Cursor requested model list â†’ returning gpt-4, gpt-3.5-turbo, claude-3-5-sonnet
ğŸ“‹ Cursor requested model list â†’ returning gpt-4, gpt-3.5-turbo, claude-3-5-sonnet
